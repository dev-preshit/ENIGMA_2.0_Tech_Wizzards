{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a1ee9938e32b45c0a5530406bc025786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29429d0e0bd04d22808d83d86e552d61",
              "IPY_MODEL_31f7e6c7497046f2b5f6497c195c9bec",
              "IPY_MODEL_9f7c31646eea410882579f4727ff2568"
            ],
            "layout": "IPY_MODEL_22f2a8ed0c2d47beb6d6625e17665783"
          }
        },
        "29429d0e0bd04d22808d83d86e552d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c85a64f7fe1c4e93a12ba970033d31da",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_26eed4517be64136878520fc5e4574f9",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "31f7e6c7497046f2b5f6497c195c9bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c872d935daea4aca8ba5c548f0634372",
            "max": 36757206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b353f6fa1d44dce83f1ab4199a7ff85",
            "value": 36757206
          }
        },
        "9f7c31646eea410882579f4727ff2568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e4da0095bf4712988d0a0a8094ab1a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f69fa0da1a154b6994fc4c136b9f4027",
            "value": "‚Äá36.8M/36.8M‚Äá[00:01&lt;00:00,‚Äá19.0MB/s]"
          }
        },
        "22f2a8ed0c2d47beb6d6625e17665783": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c85a64f7fe1c4e93a12ba970033d31da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26eed4517be64136878520fc5e4574f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c872d935daea4aca8ba5c548f0634372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b353f6fa1d44dce83f1ab4199a7ff85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33e4da0095bf4712988d0a0a8094ab1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f69fa0da1a154b6994fc4c136b9f4027": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU-UcEFcYh3v",
        "outputId": "e70957e0-6c9a-4fe6-a4a9-7ddeefe5182e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/salviohexia/isic-2019-skin-lesion-images-for-classification?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.10G/9.10G [05:08<00:00, 31.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset path: /root/.cache/kagglehub/datasets/salviohexia/isic-2019-skin-lesion-images-for-classification/versions/1\n",
            "Contents: ['ISIC_2019_Training_Metadata.csv', 'BKL', 'AK', 'ISIC_2019_Training_GroundTruth.csv', 'MEL', 'NV', 'VASC', 'BCC', 'SCC', 'DF']\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install timm -q\n",
        "\n",
        "import kagglehub\n",
        "import os, torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Download dataset\n",
        "dataset_path = kagglehub.dataset_download(\"salviohexia/isic-2019-skin-lesion-images-for-classification\")\n",
        "print(\"Dataset path:\", dataset_path)\n",
        "print(\"Contents:\", os.listdir(dataset_path))\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check classes\n",
        "CLASS_NAMES = ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n",
        "for cls in CLASS_NAMES:\n",
        "    path = os.path.join(dataset_path, cls)\n",
        "    count = len(os.listdir(path)) if os.path.exists(path) else 0\n",
        "    print(f\"{cls}: {count} images\")\n",
        "\n",
        "# Check metadata\n",
        "csv_path = os.path.join(dataset_path, \"ISIC_2019_Training_Metadata.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"\\nMetadata shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7I4T7yBYx6Z",
        "outputId": "d1722b9e-7796-4d24-8131-f6fbd04eb6cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AK: 867 images\n",
            "BCC: 3323 images\n",
            "BKL: 2624 images\n",
            "DF: 239 images\n",
            "MEL: 4522 images\n",
            "NV: 12875 images\n",
            "SCC: 628 images\n",
            "VASC: 253 images\n",
            "\n",
            "Metadata shape: (25331, 5)\n",
            "          image  age_approx anatom_site_general lesion_id     sex\n",
            "0  ISIC_0000000        55.0      anterior torso       NaN  female\n",
            "1  ISIC_0000001        30.0      anterior torso       NaN  female\n",
            "2  ISIC_0000002        60.0     upper extremity       NaN  female\n",
            "3  ISIC_0000003        30.0     upper extremity       NaN    male\n",
            "4  ISIC_0000004        80.0     posterior torso       NaN    male\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "# ‚îÄ‚îÄ Metadata Preprocessing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "def preprocess_metadata(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Rename image column to match filenames\n",
        "    df = df.rename(columns={'image': 'isic_id'})\n",
        "\n",
        "    # Age ‚Äî fill missing with median, normalize\n",
        "    df['age_approx'] = df['age_approx'].fillna(df['age_approx'].median())\n",
        "    df['age_approx'] = (df['age_approx'] - df['age_approx'].mean()) / df['age_approx'].std()\n",
        "\n",
        "    # Sex ‚Äî one hot\n",
        "    df['sex'] = df['sex'].fillna('unknown')\n",
        "    df['sex_male']    = (df['sex'] == 'male').astype(float)\n",
        "    df['sex_female']  = (df['sex'] == 'female').astype(float)\n",
        "    df['sex_unknown'] = (df['sex'] == 'unknown').astype(float)\n",
        "\n",
        "    # Anatomical site ‚Äî one hot\n",
        "    sites = ['anterior torso', 'posterior torso', 'upper extremity',\n",
        "             'lower extremity', 'head/neck', 'palms/soles', 'oral/genital']\n",
        "    df['anatom_site_general'] = df['anatom_site_general'].fillna('unknown')\n",
        "    for site in sites:\n",
        "        col = 'site_' + site.replace('/', '_').replace(' ', '_')\n",
        "        df[col] = (df['anatom_site_general'] == site).astype(float)\n",
        "\n",
        "    meta_cols = ['age_approx', 'sex_male', 'sex_female', 'sex_unknown'] + \\\n",
        "                ['site_' + s.replace('/', '_').replace(' ', '_') for s in sites]\n",
        "\n",
        "    return df, meta_cols\n",
        "\n",
        "df, meta_cols = preprocess_metadata(df)\n",
        "print(f\"‚úÖ Metadata ready | Features: {len(meta_cols)} | Columns: {meta_cols}\")\n",
        "\n",
        "# ‚îÄ‚îÄ Dataset ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "CLASS_NAMES = ['AK', 'BCC', 'BKL', 'DF', 'MEL', 'NV', 'SCC', 'VASC']\n",
        "CLASS_TO_IDX = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
        "\n",
        "class ISICDataset(Dataset):\n",
        "    def __init__(self, dataset_path, df, meta_cols, transform=None):\n",
        "        self.meta_df   = df.set_index('isic_id')\n",
        "        self.meta_cols = meta_cols\n",
        "        self.transform = transform\n",
        "        self.samples   = []\n",
        "\n",
        "        for cls in CLASS_NAMES:\n",
        "            cls_path = os.path.join(dataset_path, cls)\n",
        "            if not os.path.exists(cls_path):\n",
        "                continue\n",
        "            for img_file in os.listdir(cls_path):\n",
        "                if img_file.lower().endswith('.jpg'):\n",
        "                    img_id = img_file.replace('.jpg', '')\n",
        "                    self.samples.append({\n",
        "                        'path':  os.path.join(cls_path, img_file),\n",
        "                        'label': CLASS_TO_IDX[cls],\n",
        "                        'id':    img_id\n",
        "                    })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s   = self.samples[idx]\n",
        "        img = Image.open(s['path']).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if s['id'] in self.meta_df.index:\n",
        "            meta = torch.tensor(\n",
        "                self.meta_df.loc[s['id'], self.meta_cols].values.astype(np.float32))\n",
        "        else:\n",
        "            meta = torch.zeros(len(self.meta_cols), dtype=torch.float32)\n",
        "\n",
        "        return img, meta, s['label']\n",
        "\n",
        "# ‚îÄ‚îÄ Transforms ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "train_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomVerticalFlip(),\n",
        "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    T.RandomRotation(20),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ‚îÄ‚îÄ Load & Split ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "full_ds    = ISICDataset(dataset_path, df, meta_cols, transform=train_transform)\n",
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size   = len(full_ds) - train_size\n",
        "train_ds, val_ds = random_split(full_ds, [train_size, val_size])\n",
        "val_ds.dataset.transform = val_transform\n",
        "\n",
        "# ‚îÄ‚îÄ Weighted Sampler (fix class imbalance) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "labels       = [full_ds.samples[i]['label'] for i in range(len(full_ds))]\n",
        "counts       = Counter(labels)\n",
        "class_weights = torch.tensor([len(labels) / (len(CLASS_NAMES) * counts[i])\n",
        "                               for i in range(len(CLASS_NAMES))], dtype=torch.float32)\n",
        "sample_weights = [class_weights[l] for l in labels]\n",
        "train_indices  = train_ds.indices\n",
        "train_weights  = [sample_weights[i] for i in train_indices]\n",
        "sampler        = WeightedRandomSampler(train_weights, len(train_weights))\n",
        "\n",
        "# ‚îÄ‚îÄ DataLoaders ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"‚úÖ Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
        "print(f\"‚úÖ Class weights: {dict(zip(CLASS_NAMES, class_weights.numpy().round(2)))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C702uMpgZzIl",
        "outputId": "f0f34413-8943-409c-d673-b6a19fe81042"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Metadata ready | Features: 11 | Columns: ['age_approx', 'sex_male', 'sex_female', 'sex_unknown', 'site_anterior_torso', 'site_posterior_torso', 'site_upper_extremity', 'site_lower_extremity', 'site_head_neck', 'site_palms_soles', 'site_oral_genital']\n",
            "‚úÖ Train: 20264 | Val: 5067\n",
            "‚úÖ Class weights: {'AK': np.float32(3.65), 'BCC': np.float32(0.95), 'BKL': np.float32(1.21), 'DF': np.float32(13.25), 'MEL': np.float32(0.7), 'NV': np.float32(0.25), 'SCC': np.float32(5.04), 'VASC': np.float32(12.52)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())       # Must be True\n",
        "print(torch.cuda.get_device_name(0))   # Must show T4\n",
        "device = torch.device(\"cuda\")\n",
        "print(device)                          # cuda\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGAXc_1_av5w",
        "outputId": "49144d3f-b9a5-4fa6-a9b8-fc412305b76a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "# ‚îÄ‚îÄ Model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "class SkinCancerModel(nn.Module):\n",
        "    def __init__(self, num_classes=8, metadata_dim=11):\n",
        "        super().__init__()\n",
        "\n",
        "        # Image branch ‚Äî EfficientNet-B2\n",
        "        self.image_branch = timm.create_model('efficientnet_b2', pretrained=True)\n",
        "        image_out = self.image_branch.classifier.in_features  # 1408\n",
        "        self.image_branch.classifier = nn.Identity()\n",
        "\n",
        "        self.image_fc = nn.Sequential(\n",
        "            nn.Linear(image_out, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Metadata branch\n",
        "        self.meta_fc = nn.Sequential(\n",
        "            nn.Linear(metadata_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Fusion\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(512 + 128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, metadata):\n",
        "        img_feat  = self.image_branch(image)\n",
        "        img_feat  = self.image_fc(img_feat)\n",
        "        meta_feat = self.meta_fc(metadata)\n",
        "        fused     = torch.cat([img_feat, meta_feat], dim=1)\n",
        "        return self.fusion(fused)\n",
        "\n",
        "# ‚îÄ‚îÄ Init ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "model = SkinCancerModel(num_classes=8, metadata_dim=11).to(device)\n",
        "print(f\"‚úÖ Model ready on {device}\")\n",
        "\n",
        "# Quick sanity check\n",
        "dummy_img  = torch.randn(2, 3, 224, 224).to(device)\n",
        "dummy_meta = torch.randn(2, 11).to(device)\n",
        "out        = model(dummy_img, dummy_meta)\n",
        "print(f\"‚úÖ Output shape: {out.shape}\")  # should be torch.Size([2, 8])\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"‚úÖ Trainable parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "a1ee9938e32b45c0a5530406bc025786",
            "29429d0e0bd04d22808d83d86e552d61",
            "31f7e6c7497046f2b5f6497c195c9bec",
            "9f7c31646eea410882579f4727ff2568",
            "22f2a8ed0c2d47beb6d6625e17665783",
            "c85a64f7fe1c4e93a12ba970033d31da",
            "26eed4517be64136878520fc5e4574f9",
            "c872d935daea4aca8ba5c548f0634372",
            "9b353f6fa1d44dce83f1ab4199a7ff85",
            "33e4da0095bf4712988d0a0a8094ab1a",
            "f69fa0da1a154b6994fc4c136b9f4027"
          ]
        },
        "id": "Qi_-4GwXaBJF",
        "outputId": "408067af-3515-48cf-f3bf-7d461981052c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/36.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1ee9938e32b45c0a5530406bc025786"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model ready on cuda\n",
            "‚úÖ Output shape: torch.Size([2, 8])\n",
            "‚úÖ Trainable parameters: 8,599,434\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "# ‚îÄ‚îÄ Loss & Optimizer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': model.image_branch.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.image_fc.parameters(),     'lr': 3e-4},\n",
        "    {'params': model.meta_fc.parameters(),      'lr': 3e-4},\n",
        "    {'params': model.fusion.parameters(),       'lr': 3e-4},\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "EPOCHS    = 15\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# ‚îÄ‚îÄ Training Loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for imgs, metas, labels in train_loader:\n",
        "        imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs, metas)\n",
        "        loss    = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct    += (outputs.argmax(1) == labels).sum().item()\n",
        "        total      += labels.size(0)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    val_correct, val_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, metas, labels in val_loader:\n",
        "            imgs, metas, labels = imgs.to(device), metas.to(device), labels.to(device)\n",
        "            outputs     = model(imgs, metas)\n",
        "            val_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            val_total   += labels.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d}/{EPOCHS} | \"\n",
        "          f\"Loss: {train_loss/len(train_loader):.4f} | \"\n",
        "          f\"Train Acc: {correct/total:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.4f}\"\n",
        "          + (\" ‚úÖ saved\" if val_acc > best_val_acc else \"\"))\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "print(f\"\\nüèÜ Best Val Acc: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "Ol4qLo1kbdKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lW0dtxHbdEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}